{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6adeef-63b9-483d-bddd-3f8af334b313",
   "metadata": {},
   "source": [
    "# Extend model capabilities\n",
    "## Introduction\n",
    "* In this notebook, we will explore how to extend the capabilities of AI models by integrating external tools and APIs.\n",
    "* Extend methodologies include:\n",
    "  - Tool Use: Integrating external tools to enhance model functionality.\n",
    "  - API Calls: Leveraging APIs to fetch real-time data or perform specific tasks.\n",
    "  - MCP (Model context protocol): Using structured protocols to manage model interactions and context.\n",
    "  - Retrieval-Augmented Generation (RAG): Combining model outputs with retrieved information from external sources.\n",
    "  - Fine-tuning: Customizing models on specific datasets to improve performance on targeted tasks.\n",
    "\n",
    "## Technologies\n",
    "* OpenAI: calling api to OpenAI for getting response\n",
    "* Gradio: support in building user interface for interacting with AI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "378c5233-d4e2-4fc0-bcd0-9c2f7c83a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f42f038b-616f-400d-ab57-025687a24170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class EnvService():\n",
    "    def get_open_ai_key(self):\n",
    "        open_ai_key = os.getenv(\"OPEN_AI_KEY\")\n",
    "        if not open_ai_key:\n",
    "            print(\"OPEN AI KEY IS NOT SET!!!\")\n",
    "            return \n",
    "        return open_ai_key\n",
    "    def get_weather_api_key(self):\n",
    "        weather_api_key = os.getenv(\"WEATHER_API_KEY\")\n",
    "        if not weather_api_key:\n",
    "            print(\"WEATHER_API_KEY IS NOT SET!!!\")\n",
    "            return \n",
    "        return weather_api_key\n",
    "    def get_gemini_api_key(self):\n",
    "        gemini_ai_key = os.getenv(\"GEMINI_AI_KEY\")\n",
    "        if not gemini_ai_key:\n",
    "            print(\"GEMINI_AI_KEY IS NOT SET!!!\")\n",
    "            return\n",
    "        return gemini_ai_key\n",
    "env_service = EnvService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03e63f4e-7476-4831-b665-b0732de7dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIService:\n",
    "    model = \"gpt-4.1\"\n",
    "    def __init__(self):\n",
    "        self.init_client()\n",
    "        \n",
    "    def init_client(self):\n",
    "        self.client = OpenAI(api_key=env_service.get_open_ai_key())\n",
    "\n",
    "    def chat(self, messages):\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a4ed5a-5148-406c-9f2a-35c17c14c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.ai_service = AIService()\n",
    "    def chat(self, messages, history):\n",
    "        new_messages = copy.deepcopy(history)\n",
    "        new_messages.append({\"role\": \"user\", \"content\": messages})\n",
    "    \n",
    "        responses = self.ai_service.chat(new_messages)\n",
    "    \n",
    "        partial = \"\"\n",
    "        for chunk in responses:\n",
    "            delta = chunk.choices[0].delta\n",
    "            if delta.content is not None:\n",
    "                partial += delta.content\n",
    "                yield [\n",
    "                    {\"role\": \"assistant\", \"content\": partial}\n",
    "                ]\n",
    "\n",
    "    def render_ui(self):\n",
    "        chat_interface = gr.ChatInterface(fn=self.chat, type=\"messages\")\n",
    "        chat_interface.launch()\n",
    "    def run(self):\n",
    "        self.render_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95b4b916-2896-435c-9a33-e34f727bb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_bot = ChatBot()\n",
    "chat_bot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f41c66-0e7f-4556-9714-be9eb7f5f2db",
   "metadata": {},
   "source": [
    "# Chatbot with Tools\n",
    "\n",
    "## Introduction\n",
    "* Using tools to improve the chat bot that can create image, audio\n",
    "* Using tools to imptove the chat bot can provide current weather at a location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d34e8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"Location name, e.g. London\"},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"generate_image\",\n",
    "            \"description\": \"Generate an image based on a text prompt\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"prompt\": {\"type\": \"string\", \"description\": \"What to generate\"},\n",
    "                },\n",
    "                \"required\": [\"prompt\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c84fb708-140b-4dea-9a97-5354a3a2eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class WeatherService:\n",
    "    def __init__(self):\n",
    "        self.api_key = env_service.get_weather_api_key()\n",
    "        self.api_url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "    def get_weather(self, location: str) -> str:\n",
    "        params = {\n",
    "            \"q\": location,\n",
    "            \"appid\": self.api_key,\n",
    "            \"units\": \"metric\"  # return ¬∞C instead of Kelvin\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(self.api_url, params=params)\n",
    "            data = response.json()\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                return f\"Error: {data.get('message', 'Unable to fetch weather')}\"\n",
    "\n",
    "            # Parse relevant info\n",
    "            city = data.get(\"name\", location)\n",
    "            country = data.get(\"sys\", {}).get(\"country\", \"\")\n",
    "            weather_main = data[\"weather\"][0][\"main\"]\n",
    "            weather_desc = data[\"weather\"][0][\"description\"]\n",
    "            temp = data[\"main\"][\"temp\"]\n",
    "            feels_like = data[\"main\"][\"feels_like\"]\n",
    "            humidity = data[\"main\"][\"humidity\"]\n",
    "            wind_speed = data[\"wind\"][\"speed\"]\n",
    "\n",
    "            return (\n",
    "                f\"Weather in {city}, {country}:\\n\"\n",
    "                f\"- Condition: {weather_main} ({weather_desc})\\n\"\n",
    "                f\"- Temperature: {temp:.1f}¬∞C (feels like {feels_like:.1f}¬∞C)\\n\"\n",
    "                f\"- Humidity: {humidity}%\\n\"\n",
    "                f\"- Wind speed: {wind_speed} m/s\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching weather: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa1075ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "class AIServiceWithTools(AIService):\n",
    "    model_gen_image = 'gemini-2.0-flash-preview-image-generation'\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tools = tools\n",
    "        self.client_gemini = genai.Client(\n",
    "            api_key=env_service.get_gemini_api_key(),\n",
    "        )\n",
    "    def chat_with_tools(self, messages):\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            tools=self.tools,\n",
    "            stream=True\n",
    "        )\n",
    "        return responses\n",
    "    def generate_image(self, prompt, save_path):\n",
    "        response = self.client_gemini.models.generate_content(\n",
    "            model=\"gemini-2.0-flash-preview-image-generation\",\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "            response_modalities=['TEXT', 'IMAGE']\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for part in response.candidates[0].content.parts:\n",
    "            if part.text is not None:\n",
    "                print(part.text)\n",
    "            elif part.inline_data is not None:\n",
    "                image = Image.open(BytesIO((part.inline_data.data)))\n",
    "                image.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b44935a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import gradio as gr\n",
    "\n",
    "class ChatBotWithTools(ChatBot):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ai_service = AIServiceWithTools()\n",
    "        self.weather_service = WeatherService()\n",
    "\n",
    "    # --- Utility: clean history before sending to model ---\n",
    "    def sanitize_history_for_model(self, history):\n",
    "        cleaned = []\n",
    "        for msg in history:\n",
    "            role = msg.get(\"role\")\n",
    "            content = msg.get(\"content\")\n",
    "\n",
    "            # Skip image dicts from Gradio\n",
    "            if isinstance(content, dict):\n",
    "                img_desc = content.get(\"path\", \"\")\n",
    "                if img_desc:\n",
    "                    cleaned.append({\n",
    "                        \"role\": role,\n",
    "                        \"content\": f\"[Image shown to user: {img_desc}]\"\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            # Extract text from multimodal lists if any\n",
    "            if isinstance(content, list):\n",
    "                text_parts = []\n",
    "                for c in content:\n",
    "                    if isinstance(c, dict) and c.get(\"type\") == \"text\":\n",
    "                        text_parts.append(c.get(\"text\", \"\"))\n",
    "                    elif isinstance(c, str):\n",
    "                        text_parts.append(c)\n",
    "                content = \"\\n\".join(text_parts)\n",
    "\n",
    "            # Convert tuples to string\n",
    "            if isinstance(content, tuple):\n",
    "                content = \" \".join(map(str, content))\n",
    "\n",
    "            # Skip empty messages\n",
    "            if not content or not isinstance(content, str):\n",
    "                continue\n",
    "\n",
    "            cleaned.append({\"role\": role, \"content\": content})\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "    # --- Main chat handler ---\n",
    "    def chat_with_tools(self, messages, history):\n",
    "        # Step 1: sanitize previous conversation before sending to model\n",
    "        safe_history = self.sanitize_history_for_model(history)\n",
    "\n",
    "        # Step 2: append current user message (handle multimodal format from Gradio)\n",
    "        user_content = messages\n",
    "        if isinstance(messages, list):\n",
    "            # Extract text from multimodal message format\n",
    "            text_parts = [m.get(\"text\", \"\") for m in messages if isinstance(m, dict) and m.get(\"type\") == \"text\"]\n",
    "            user_content = \"\\n\".join(text_parts) if text_parts else messages\n",
    "        elif isinstance(messages, dict):\n",
    "            # Handle dict format (e.g., file uploads)\n",
    "            user_content = str(messages)\n",
    "        \n",
    "        new_messages = copy.deepcopy(safe_history)\n",
    "        new_messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "        # Step 3: stream model responses\n",
    "        print(\"Sending to model...\", new_messages)\n",
    "        responses = self.ai_service.chat_with_tools(new_messages)\n",
    "\n",
    "        partial = \"\"\n",
    "        tool_call_data = {}\n",
    "\n",
    "        for chunk in responses:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            # --- Stream normal text ---\n",
    "            if delta.content is not None:\n",
    "                partial += delta.content\n",
    "                yield [{\"role\": \"assistant\", \"content\": partial}]\n",
    "\n",
    "            # --- Collect streamed tool calls ---\n",
    "            if delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls:\n",
    "                    idx = tool_call.index\n",
    "                    fn_name = tool_call.function.name\n",
    "                    fn_args_part = tool_call.function.arguments\n",
    "\n",
    "                    if idx not in tool_call_data:\n",
    "                        tool_call_data[idx] = {\"name\": fn_name, \"args\": \"\"}\n",
    "\n",
    "                    if fn_name:\n",
    "                        tool_call_data[idx][\"name\"] = fn_name\n",
    "                    if fn_args_part:\n",
    "                        tool_call_data[idx][\"args\"] += fn_args_part\n",
    "\n",
    "        # Step 4: Execute tools after streaming completes\n",
    "        for idx, tool in tool_call_data.items():\n",
    "            fn_name = tool[\"name\"]\n",
    "            args_str = tool[\"args\"]\n",
    "\n",
    "            try:\n",
    "                args = json.loads(args_str)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to parse tool args:\", args_str, e)\n",
    "                args = {}\n",
    "\n",
    "            # --- Weather tool ---\n",
    "            if fn_name == \"get_weather\":\n",
    "                tool_result = self.weather_service.get_weather(**args)\n",
    "                yield [{\"role\": \"assistant\", \"content\": tool_result}]\n",
    "\n",
    "            # --- Image generation tool ---\n",
    "            elif fn_name == \"generate_image\":\n",
    "                prompt = args.get(\"prompt\", \"\")\n",
    "                save_path = \"generate_image.png\"\n",
    "                self.ai_service.generate_image(prompt, save_path)\n",
    "\n",
    "                # Step 4a: Tell user what image was made\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Here is your image for: {prompt}\"}]\n",
    "\n",
    "                # Step 4b: Show image in Gradio\n",
    "                yield [{\"role\": \"assistant\", \"content\": {\"path\": save_path, \"mime_type\": \"image/png\"}}]\n",
    "\n",
    "                # Step 4c: Add text reference so model can remember next time\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"[Generated image for: {prompt}]\"})\n",
    "\n",
    "            else:\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Unknown function call: {fn_name}\"}]\n",
    "\n",
    "    # --- Gradio UI setup ---\n",
    "    def render_ui(self):\n",
    "        chat_interface = gr.ChatInterface(fn=self.chat_with_tools, type=\"messages\")\n",
    "        chat_interface.launch()\n",
    "\n",
    "    def run(self):\n",
    "        self.render_ui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a489c8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_bot_with_tools = ChatBotWithTools()\n",
    "chat_bot_with_tools.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a8645",
   "metadata": {},
   "source": [
    "## ChatBot with MCP\n",
    "\n",
    "### Introduction\n",
    "* Extending the base ChatBot class to support tool usage via MCP\n",
    "* Give example of setting calendar for scheduling study time (list available subjects, set study time, view schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9e779dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIServiceWithMCP(AIServiceWithTools):\n",
    "    def chat_with_mcp(self, messages):\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=True\n",
    "        )\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6edb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Optional\n",
    "from contextlib import AsyncExitStack\n",
    "import nest_asyncio\n",
    "\n",
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "\n",
    "# Allow nested event loops (required for Jupyter + Gradio + MCP)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class ChatBotWithMCP(ChatBotWithTools):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mcp_session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.mcp_tools = []\n",
    "        self.ai_service_with_mcp = AIServiceWithMCP()\n",
    "        self.event_loop = None  # Store the loop where MCP session was created\n",
    "    \n",
    "    async def start_mcp_session(self):\n",
    "        \"\"\"Connect to SSE-based MCP server and keep session alive\"\"\"\n",
    "        server_url = \"http://localhost:8000/sse\"\n",
    "        \n",
    "        # Store the event loop where we create the session\n",
    "        self.event_loop = asyncio.get_event_loop()\n",
    "        \n",
    "        # Keep the context alive by storing in exit_stack\n",
    "        read, write = await self.exit_stack.enter_async_context(sse_client(server_url))\n",
    "        self.mcp_session = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
    "        \n",
    "        await self.mcp_session.initialize()\n",
    "        \n",
    "        # List available tools\n",
    "        response = await self.mcp_session.list_tools()\n",
    "        print(\"\\nConnected to MCP server with tools:\", [tool.name for tool in response.tools])\n",
    "        self.mcp_tools = response.tools\n",
    "        \n",
    "        return self.mcp_session\n",
    "\n",
    "    def convert_mcp_tools_to_openai_format(self):\n",
    "        \"\"\"Convert MCP tools to OpenAI function calling format.\"\"\"\n",
    "        openai_tools = []\n",
    "        for tool in self.mcp_tools:\n",
    "            # Get input schema or create default\n",
    "            input_schema = tool.inputSchema if hasattr(tool, 'inputSchema') and tool.inputSchema else {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            }\n",
    "            \n",
    "            # Ensure input_schema has required fields\n",
    "            if not isinstance(input_schema, dict):\n",
    "                input_schema = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "            if \"type\" not in input_schema:\n",
    "                input_schema[\"type\"] = \"object\"\n",
    "            if \"properties\" not in input_schema:\n",
    "                input_schema[\"properties\"] = {}\n",
    "            \n",
    "            openai_tool = {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description if hasattr(tool, 'description') and tool.description else f\"Execute {tool.name}\",\n",
    "                    \"parameters\": input_schema\n",
    "                }\n",
    "            }\n",
    "            openai_tools.append(openai_tool)\n",
    "            \n",
    "        print(f\"Converted {len(openai_tools)} MCP tools to OpenAI format\")\n",
    "        # Merge with existing tools (weather, image generation)\n",
    "        all_tools = tools + openai_tools  # Combine built-in and MCP tools\n",
    "        print(f\"Total tools available: {len(all_tools)} (Built-in: {len(tools)}, MCP: {len(openai_tools)})\")\n",
    "        return all_tools\n",
    "\n",
    "    async def execute_mcp_tool(self, tool_name: str, arguments: dict) -> str:\n",
    "        \"\"\"Execute a tool on the MCP server.\"\"\"\n",
    "        if not self.mcp_session:\n",
    "            return \"Error: MCP session not initialized\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"Executing MCP tool: {tool_name} with args: {arguments}\")\n",
    "            result = await self.mcp_session.call_tool(tool_name, arguments=arguments)\n",
    "            print(f\"MCP tool result: {result}\")\n",
    "            \n",
    "            # Extract text content from result\n",
    "            if hasattr(result, 'content') and len(result.content) > 0:\n",
    "                content_item = result.content[0]\n",
    "                if hasattr(content_item, 'text'):\n",
    "                    return content_item.text\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_details = traceback.format_exc()\n",
    "            print(f\"Error executing MCP tool: {error_details}\")\n",
    "            return f\"Error executing tool {tool_name}: {str(e)}\"\n",
    "\n",
    "    def chat_with_mcp_tool(self, messages, history):\n",
    "        \"\"\"Chat method that uses MCP server tools with OpenAI function calling.\"\"\"\n",
    "        # Sanitize history\n",
    "        safe_history = self.sanitize_history_for_model(history)\n",
    "        \n",
    "        # Prepare user message\n",
    "        user_content = messages\n",
    "        if isinstance(messages, list):\n",
    "            text_parts = [m.get(\"text\", \"\") for m in messages if isinstance(m, dict) and m.get(\"type\") == \"text\"]\n",
    "            user_content = \"\\n\".join(text_parts) if text_parts else messages\n",
    "        elif isinstance(messages, dict):\n",
    "            user_content = str(messages)\n",
    "        \n",
    "        new_messages = copy.deepcopy(safe_history)\n",
    "        new_messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "        # Get all tools (MCP + built-in)\n",
    "        all_tools = self.convert_mcp_tools_to_openai_format()\n",
    "        \n",
    "        # Call OpenAI with combined tools\n",
    "        responses = self.ai_service.client.chat.completions.create(\n",
    "            model=self.ai_service.model,\n",
    "            messages=new_messages,\n",
    "            tools=all_tools,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        partial = \"\"\n",
    "        tool_call_data = {}\n",
    "\n",
    "        for chunk in responses:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            # Stream text content\n",
    "            if delta.content is not None:\n",
    "                partial += delta.content\n",
    "                yield [{\"role\": \"assistant\", \"content\": partial}]\n",
    "\n",
    "            # Collect tool calls\n",
    "            if delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls:\n",
    "                    idx = tool_call.index\n",
    "                    fn_name = tool_call.function.name\n",
    "                    fn_args_part = tool_call.function.arguments\n",
    "\n",
    "                    if idx not in tool_call_data:\n",
    "                        tool_call_data[idx] = {\"name\": fn_name, \"args\": \"\"}\n",
    "\n",
    "                    if fn_name:\n",
    "                        tool_call_data[idx][\"name\"] = fn_name\n",
    "                    if fn_args_part:\n",
    "                        tool_call_data[idx][\"args\"] += fn_args_part\n",
    "\n",
    "        # Execute collected tool calls\n",
    "        for idx, tool in tool_call_data.items():\n",
    "            fn_name = tool[\"name\"]\n",
    "            args_str = tool[\"args\"]\n",
    "\n",
    "            try:\n",
    "                args = json.loads(args_str)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to parse tool args:\", args_str, e)\n",
    "                args = {}\n",
    "\n",
    "            # Check if it's an MCP tool\n",
    "            mcp_tool_names = [t.name for t in self.mcp_tools]\n",
    "            if fn_name in mcp_tool_names:\n",
    "                # Execute MCP tool using the same event loop where session was created\n",
    "                try:\n",
    "                    if self.event_loop and self.event_loop.is_running():\n",
    "                        # Use the same loop that created the MCP session\n",
    "                        future = asyncio.run_coroutine_threadsafe(\n",
    "                            self.execute_mcp_tool(fn_name, args),\n",
    "                            self.event_loop\n",
    "                        )\n",
    "                        tool_result = future.result(timeout=30)\n",
    "                    else:\n",
    "                        # Fallback if loop is not running\n",
    "                        tool_result = asyncio.run(self.execute_mcp_tool(fn_name, args))\n",
    "                    \n",
    "                    # Ensure we got a valid result\n",
    "                    if tool_result is None:\n",
    "                        tool_result = \"No response from MCP tool\"\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    import traceback\n",
    "                    tool_result = f\"Error executing MCP tool: {str(e)}\\n{traceback.format_exc()}\"\n",
    "                    \n",
    "                yield [{\"role\": \"assistant\", \"content\": tool_result}]\n",
    "            \n",
    "            # Handle built-in tools\n",
    "            elif fn_name == \"get_weather\":\n",
    "                tool_result = self.weather_service.get_weather(**args)\n",
    "                yield [{\"role\": \"assistant\", \"content\": tool_result}]\n",
    "\n",
    "            elif fn_name == \"generate_image\":\n",
    "                prompt = args.get(\"prompt\", \"\")\n",
    "                save_path = \"generate_image.png\"\n",
    "                self.ai_service.generate_image(prompt, save_path)\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Here is your image for: {prompt}\"}]\n",
    "                yield [{\"role\": \"assistant\", \"content\": {\"path\": save_path, \"mime_type\": \"image/png\"}}]\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"[Generated image for: {prompt}]\"})\n",
    "\n",
    "            else:\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Unknown function call: {fn_name}\"}]\n",
    "\n",
    "    # --- Gradio UI setup ---\n",
    "    def render_ui_with_mcp(self):\n",
    "        \"\"\"Render UI that uses MCP tools.\"\"\"\n",
    "        chat_interface = gr.ChatInterface(fn=self.chat_with_mcp_tool, type=\"messages\")\n",
    "        chat_interface.launch()\n",
    "\n",
    "    async def run_with_mcp_async(self):\n",
    "        \"\"\"Async wrapper to initialize MCP before launching UI\"\"\"\n",
    "        await self.start_mcp_session()\n",
    "        self.render_ui_with_mcp()\n",
    "\n",
    "    def run_with_mcp(self):\n",
    "        \"\"\"Run chatbot with MCP tools.\"\"\"\n",
    "        # Initialize MCP session before starting UI\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # Jupyter notebook case\n",
    "            asyncio.ensure_future(self.run_with_mcp_async())\n",
    "        else:\n",
    "            loop.run_until_complete(self.run_with_mcp_async())\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanup MCP session when done\"\"\"\n",
    "        await self.exit_stack.aclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f61c854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and test MCP connection\n",
    "chat_bot_with_mcp = ChatBotWithMCP()\n",
    "chat_bot_with_mcp.run_with_mcp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758df73",
   "metadata": {},
   "source": [
    "# ChatBot with RAG\n",
    "## Introduction\n",
    "* Using Retrieval-Augmented Generation (RAG) to enhance chatbot responses with relevant external information.\n",
    "* Example: Building a chatbot that can answer questions about a specific document or dataset by retrieving relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64f00e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to path for importing embedded module\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'src':\n",
    "    sys.path.insert(0, str(current_dir))\n",
    "else:\n",
    "    src_dir = current_dir / 'src'\n",
    "    if src_dir.exists():\n",
    "        sys.path.insert(0, str(src_dir))\n",
    "\n",
    "from embedded import KnowledgeBaseVectorizer\n",
    "\n",
    "class ChatBotWithRAG(ChatBotWithMCP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize RAG-specific components with pgvector\n",
    "        self.vectorizer = self._init_vectorizer()\n",
    "    \n",
    "    def _init_vectorizer(self):\n",
    "        \"\"\"Initialize the KnowledgeBaseVectorizer for RAG retrieval\"\"\"\n",
    "        db_config = {\n",
    "            'host': os.getenv('DB_HOST', 'localhost'),\n",
    "            'port': os.getenv('DB_PORT', '5432'),\n",
    "            'database': os.getenv('DB_NAME', 'ai_chatbot'),\n",
    "            'user': os.getenv('DB_USER', 'postgres'),\n",
    "            'password': os.getenv('DB_PASSWORD', 'postgres')\n",
    "        }\n",
    "        \n",
    "        # Path to knowledge base - navigate from current directory\n",
    "        current_path = Path.cwd()\n",
    "        if current_path.name == 'src':\n",
    "            knowledge_base_path = current_path.parent / 'knowledge-base'\n",
    "        else:\n",
    "            knowledge_base_path = current_path / 'knowledge-base'\n",
    "        \n",
    "        vectorizer = KnowledgeBaseVectorizer(str(knowledge_base_path), db_config)\n",
    "        vectorizer.connect_db()\n",
    "        print(\"‚úì RAG vectorizer initialized and connected to database\")\n",
    "        \n",
    "        return vectorizer\n",
    "    \n",
    "    def retrieve_relevant_docs(self, query: str, top_k: int = 3) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from pgvector based on the query\n",
    "        \n",
    "        Args:\n",
    "            query: User's question/query\n",
    "            top_k: Number of relevant documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant document contents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use pgvector similarity search to find relevant content\n",
    "            results = self.vectorizer.search_similar(\n",
    "                query=query,\n",
    "                top_k=top_k,\n",
    "                expand_query=True,  # Use intelligent query expansion\n",
    "                min_similarity=0.3   # Only return reasonably relevant results\n",
    "            )\n",
    "            \n",
    "            # Extract content from results and clean metadata prefixes\n",
    "            relevant_docs = []\n",
    "            for result in results:\n",
    "                content = result['content']\n",
    "                # Remove metadata prefix for cleaner context\n",
    "                if '\\n\\n' in content:\n",
    "                    content = content.split('\\n\\n', 1)[-1]\n",
    "                \n",
    "                # Add source information\n",
    "                source_info = f\"[Source: {result['file_path']} - Similarity: {result['similarity']:.2f}]\"\n",
    "                relevant_docs.append(f\"{source_info}\\n{content}\")\n",
    "            \n",
    "            return relevant_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving documents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def chat_with_rag(self, messages, history):\n",
    "        # Step 1: Sanitize history\n",
    "        safe_history = self.sanitize_history_for_model(history)\n",
    "        \n",
    "        # Step 2: Prepare user message\n",
    "        user_content = messages\n",
    "        if isinstance(messages, list):\n",
    "            text_parts = [m.get(\"text\", \"\") for m in messages if isinstance(m, dict) and m.get(\"type\") == \"text\"]\n",
    "            user_content = \"\\n\".join(text_parts) if text_parts else messages\n",
    "        elif isinstance(messages, dict):\n",
    "            user_content = str(messages)\n",
    "        \n",
    "        new_messages = copy.deepcopy(safe_history)\n",
    "        \n",
    "        # Step 3: Retrieve relevant documents from pgvector\n",
    "        print(f\"üîç Retrieving relevant documents for: {user_content}\")\n",
    "        relevant_docs = self.retrieve_relevant_docs(user_content, top_k=3)\n",
    "        \n",
    "        # Step 4: Augment user message with retrieved docs if found\n",
    "        if relevant_docs:\n",
    "            docs_content = \"\\n\\n---\\n\\n\".join(relevant_docs)\n",
    "            augmented_message = (\n",
    "                f\"User Question: {user_content}\\n\\n\"\n",
    "                f\"Relevant Context from Knowledge Base:\\n{docs_content}\\n\\n\"\n",
    "                f\"Please answer the user's question using the provided context. \"\n",
    "                f\"If the context is relevant, use it to provide accurate information. \"\n",
    "                f\"If the context is not relevant, answer based on your general knowledge.\"\n",
    "            )\n",
    "            print(f\"‚úì Found {len(relevant_docs)} relevant documents\")\n",
    "        else:\n",
    "            augmented_message = user_content\n",
    "            print(\"‚ö†Ô∏è  No relevant documents found, using general knowledge\")\n",
    "        \n",
    "        new_messages.append({\"role\": \"user\", \"content\": augmented_message})\n",
    "        \n",
    "        # Step 5: Get all tools (MCP + built-in)\n",
    "        all_tools = self.convert_mcp_tools_to_openai_format() if self.mcp_tools else tools\n",
    "        \n",
    "        # Step 6: Call chat with tools\n",
    "        responses = self.ai_service.client.chat.completions.create(\n",
    "            model=self.ai_service.model,\n",
    "            messages=new_messages,\n",
    "            tools=all_tools,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        partial = \"\"\n",
    "        tool_call_data = {}\n",
    "\n",
    "        for chunk in responses:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            # Stream text content\n",
    "            if delta.content is not None:\n",
    "                partial += delta.content\n",
    "                yield [{\"role\": \"assistant\", \"content\": partial}]\n",
    "\n",
    "            # Collect tool calls\n",
    "            if delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls:\n",
    "                    idx = tool_call.index\n",
    "                    fn_name = tool_call.function.name\n",
    "                    fn_args_part = tool_call.function.arguments\n",
    "\n",
    "                    if idx not in tool_call_data:\n",
    "                        tool_call_data[idx] = {\"name\": fn_name, \"args\": \"\"}\n",
    "\n",
    "                    if fn_name:\n",
    "                        tool_call_data[idx][\"name\"] = fn_name\n",
    "                    if fn_args_part:\n",
    "                        tool_call_data[idx][\"args\"] += fn_args_part\n",
    "\n",
    "        # Execute collected tool calls\n",
    "        for idx, tool in tool_call_data.items():\n",
    "            fn_name = tool[\"name\"]\n",
    "            args_str = tool[\"args\"]\n",
    "\n",
    "            try:\n",
    "                args = json.loads(args_str)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to parse tool args:\", args_str, e)\n",
    "                args = {}\n",
    "\n",
    "            # Check if it's an MCP tool\n",
    "            if self.mcp_tools:\n",
    "                mcp_tool_names = [t.name for t in self.mcp_tools]\n",
    "                if fn_name in mcp_tool_names:\n",
    "                    try:\n",
    "                        if self.event_loop and self.event_loop.is_running():\n",
    "                            future = asyncio.run_coroutine_threadsafe(\n",
    "                                self.execute_mcp_tool(fn_name, args),\n",
    "                                self.event_loop\n",
    "                            )\n",
    "                            tool_result = future.result(timeout=30)\n",
    "                        else:\n",
    "                            tool_result = asyncio.run(self.execute_mcp_tool(fn_name, args))\n",
    "                        \n",
    "                        if tool_result is None:\n",
    "                            tool_result = \"No response from MCP tool\"\n",
    "                    except Exception as e:\n",
    "                        import traceback\n",
    "                        tool_result = f\"Error executing MCP tool: {str(e)}\\n{traceback.format_exc()}\"\n",
    "                    \n",
    "                    yield [{\"role\": \"assistant\", \"content\": tool_result}]\n",
    "                    continue\n",
    "            \n",
    "            # Handle built-in tools\n",
    "            if fn_name == \"get_weather\":\n",
    "                tool_result = self.weather_service.get_weather(**args)\n",
    "                yield [{\"role\": \"assistant\", \"content\": tool_result}]\n",
    "\n",
    "            elif fn_name == \"generate_image\":\n",
    "                prompt = args.get(\"prompt\", \"\")\n",
    "                save_path = \"generate_image.png\"\n",
    "                self.ai_service.generate_image(prompt, save_path)\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Here is your image for: {prompt}\"}]\n",
    "                yield [{\"role\": \"assistant\", \"content\": {\"path\": save_path, \"mime_type\": \"image/png\"}}]\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"[Generated image for: {prompt}]\"})\n",
    "\n",
    "            else:\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Unknown function call: {fn_name}\"}]\n",
    "    \n",
    "    def render_ui_with_rag(self):\n",
    "        \"\"\"Render UI that uses RAG with pgvector.\"\"\"\n",
    "        chat_interface = gr.ChatInterface(\n",
    "            fn=self.chat_with_rag, \n",
    "            type=\"messages\",\n",
    "            title=\"AI Chatbot with RAG (pgvector)\",\n",
    "            description=\"Ask questions and I'll search the knowledge base for relevant information!\"\n",
    "        )\n",
    "        chat_interface.launch()\n",
    "\n",
    "    async def run_with_rag_async(self):\n",
    "        \"\"\"Async wrapper to initialize MCP (if available) before launching UI\"\"\"\n",
    "        try:\n",
    "            await self.start_mcp_session()\n",
    "            print(\"‚úì MCP session started\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  MCP not available: {e}\")\n",
    "            print(\"Continuing with RAG only...\")\n",
    "        \n",
    "        self.render_ui_with_rag()\n",
    "\n",
    "    def run_with_rag(self):\n",
    "        \"\"\"Run chatbot with RAG and optional MCP tools.\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # Jupyter notebook case\n",
    "            asyncio.ensure_future(self.run_with_rag_async())\n",
    "        else:\n",
    "            loop.run_until_complete(self.run_with_rag_async())\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources\"\"\"\n",
    "        if self.vectorizer:\n",
    "            self.vectorizer.close()\n",
    "        super().cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a331169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connected to PostgreSQL database\n",
      "‚úì RAG vectorizer initialized and connected to database\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  MCP not available: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "Continuing with RAG only...\n",
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Retrieving relevant documents for: Hello do you know Oven company?\n",
      "Total embeddings in database: 11\n",
      "Expanded query: 'Company information and details: Hello do you know Oven company?'\n",
      "Generating embedding for query: 'Company information and details: Hello do you know Oven company?'\n",
      "‚úì Query embedding generated (dimension: 1536)\n",
      "‚úì Found 3 similar results\n",
      "  Top result similarity: 0.7200 (distance: 0.2800)\n",
      "  Unique entities: 3\n",
      "‚úì Found 3 relevant documents\n",
      "‚úì Query embedding generated (dimension: 1536)\n",
      "‚úì Found 3 similar results\n",
      "  Top result similarity: 0.7200 (distance: 0.2800)\n",
      "  Unique entities: 3\n",
      "‚úì Found 3 relevant documents\n",
      "üîç Retrieving relevant documents for: Have you know members in Oven and which roles of them?\n",
      "Total embeddings in database: 11\n",
      "Expanded query: 'Company information and details: Have you know members in Oven and which roles of them?'\n",
      "Generating embedding for query: 'Company information and details: Have you know members in Oven and which roles of them?'\n",
      "üîç Retrieving relevant documents for: Have you know members in Oven and which roles of them?\n",
      "Total embeddings in database: 11\n",
      "Expanded query: 'Company information and details: Have you know members in Oven and which roles of them?'\n",
      "Generating embedding for query: 'Company information and details: Have you know members in Oven and which roles of them?'\n",
      "‚úì Query embedding generated (dimension: 1536)\n",
      "‚úì Found 3 similar results\n",
      "  Top result similarity: 0.6505 (distance: 0.3495)\n",
      "  Unique entities: 3\n",
      "‚úì Found 3 relevant documents\n",
      "‚úì Query embedding generated (dimension: 1536)\n",
      "‚úì Found 3 similar results\n",
      "  Top result similarity: 0.6505 (distance: 0.3495)\n",
      "  Unique entities: 3\n",
      "‚úì Found 3 relevant documents\n",
      "üîç Retrieving relevant documents for: Who is responsible in backend development?\n",
      "Total embeddings in database: 11\n",
      "Expanded query: 'Employee profile and information about Who is responsible in backend development?'\n",
      "Generating embedding for query: 'Employee profile and information about Who is responsible in backend development?'\n",
      "üîç Retrieving relevant documents for: Who is responsible in backend development?\n",
      "Total embeddings in database: 11\n",
      "Expanded query: 'Employee profile and information about Who is responsible in backend development?'\n",
      "Generating embedding for query: 'Employee profile and information about Who is responsible in backend development?'\n",
      "‚úì Query embedding generated (dimension: 1536)\n",
      "‚úì Found 3 similar results\n",
      "  Top result similarity: 0.5155 (distance: 0.4845)\n",
      "  Unique entities: 3\n",
      "‚úì Found 3 relevant documents\n",
      "‚úì Query embedding generated (dimension: 1536)\n",
      "‚úì Found 3 similar results\n",
      "  Top result similarity: 0.5155 (distance: 0.4845)\n",
      "  Unique entities: 3\n",
      "‚úì Found 3 relevant documents\n"
     ]
    }
   ],
   "source": [
    "# Initialize and run RAG chatbot\n",
    "chat_bot_with_rag = ChatBotWithRAG()\n",
    "chat_bot_with_rag.run_with_rag()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46274a5",
   "metadata": {},
   "source": [
    "## How RAG Integration Works\n",
    "\n",
    "### Features:\n",
    "1. **pgvector Integration**: Uses the `KnowledgeBaseVectorizer` from `embedded.py` to search relevant documents\n",
    "2. **Semantic Search**: Automatically finds the most relevant information from your knowledge base using embeddings\n",
    "3. **Context Augmentation**: Enriches user queries with relevant documents before sending to the AI model\n",
    "4. **Intelligent Query Expansion**: Automatically expands queries for better semantic matching\n",
    "5. **Similarity Filtering**: Only includes documents with similarity > 0.3 to ensure relevance\n",
    "\n",
    "### Example Queries:\n",
    "- \"Who is a Backend Engineer?\" ‚Üí Searches employee profiles and returns relevant matches\n",
    "- \"Tell me about Long\" ‚Üí Retrieves Long's profile information\n",
    "- \"What skills does a Frontend Engineer need?\" ‚Üí Finds relevant employee profiles with those skills\n",
    "- \"What does Oven do?\" ‚Üí Searches company information\n",
    "\n",
    "### How It Works:\n",
    "1. User asks a question\n",
    "2. System searches pgvector for relevant documents (top 3 results)\n",
    "3. Augments the user's question with retrieved context\n",
    "4. Sends augmented query to AI model\n",
    "5. AI responds using both the context and its general knowledge\n",
    "\n",
    "### Requirements:\n",
    "- PostgreSQL with pgvector extension installed\n",
    "- Embeddings already generated (run `python src/embedded.py` first)\n",
    "- Environment variables set in `.env` file (DB credentials, OpenAI API key)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
