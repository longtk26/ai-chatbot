{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6adeef-63b9-483d-bddd-3f8af334b313",
   "metadata": {},
   "source": [
    "# Extend model capabilities\n",
    "## Introduction\n",
    "* In this notebook, we will explore how to extend the capabilities of AI models by integrating external tools, MCP and RAG.\n",
    "* Extend methodologies include:\n",
    "  - Tool Use: Integrating external tools to enhance model functionality.\n",
    "  - MCP (Model context protocol): Using structured protocols to manage model interactions and context.\n",
    "  - Retrieval-Augmented Generation (RAG): Combining model outputs with retrieved information from external sources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "378c5233-d4e2-4fc0-bcd0-9c2f7c83a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/backend/research/ai-chatbot/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import gradio as gr\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42f038b-616f-400d-ab57-025687a24170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "class EnvService():\n",
    "    def get_open_ai_key(self):\n",
    "        open_ai_key = os.getenv(\"OPEN_AI_KEY\")\n",
    "        if not open_ai_key:\n",
    "            print(\"OPEN AI KEY IS NOT SET!!!\")\n",
    "            return \n",
    "        return open_ai_key\n",
    "    def get_weather_api_key(self):\n",
    "        weather_api_key = os.getenv(\"WEATHER_API_KEY\")\n",
    "        if not weather_api_key:\n",
    "            print(\"WEATHER_API_KEY IS NOT SET!!!\")\n",
    "            return \n",
    "        return weather_api_key\n",
    "    def get_gemini_api_key(self):\n",
    "        gemini_ai_key = os.getenv(\"GEMINI_AI_KEY\")\n",
    "        if not gemini_ai_key:\n",
    "            print(\"GEMINI_AI_KEY IS NOT SET!!!\")\n",
    "            return\n",
    "        return gemini_ai_key\n",
    "env_service = EnvService()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e63f4e-7476-4831-b665-b0732de7dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIService:\n",
    "    model = \"gpt-4.1\"\n",
    "    def __init__(self):\n",
    "        self.init_client()\n",
    "        \n",
    "    def init_client(self):\n",
    "        self.client = OpenAI(api_key=env_service.get_open_ai_key())\n",
    "\n",
    "    def chat(self, messages):\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            stream=True\n",
    "        )\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14a4ed5a-5148-406c-9f2a-35c17c14c793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.ai_service = AIService()\n",
    "    def chat(self, messages, history):\n",
    "        new_messages = copy.deepcopy(history)\n",
    "        new_messages.append({\"role\": \"user\", \"content\": messages})\n",
    "    \n",
    "        responses = self.ai_service.chat(new_messages)\n",
    "    \n",
    "        partial = \"\"\n",
    "        for chunk in responses:\n",
    "            delta = chunk.choices[0].delta\n",
    "            if delta.content is not None:\n",
    "                partial += delta.content\n",
    "                yield [\n",
    "                    {\"role\": \"assistant\", \"content\": partial}\n",
    "                ]\n",
    "\n",
    "    def render_ui(self):\n",
    "        chat_interface = gr.ChatInterface(fn=self.chat, type=\"messages\")\n",
    "        chat_interface.launch()\n",
    "    def run(self):\n",
    "        self.render_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b4b916-2896-435c-9a33-e34f727bb74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_bot = ChatBot()\n",
    "chat_bot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f41c66-0e7f-4556-9714-be9eb7f5f2db",
   "metadata": {},
   "source": [
    "# Chatbot with Tools\n",
    "\n",
    "* Using tools to improve the chat bot that can create image\n",
    "* Using tools to imptove the chat bot can provide current weather at a location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d34e8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"Location name, e.g. London\"},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"generate_image\",\n",
    "            \"description\": \"Generate an image based on a text prompt\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"prompt\": {\"type\": \"string\", \"description\": \"What to generate\"},\n",
    "                },\n",
    "                \"required\": [\"prompt\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84fb708-140b-4dea-9a97-5354a3a2eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "class WeatherService:\n",
    "    def __init__(self):\n",
    "        self.api_key = env_service.get_weather_api_key()\n",
    "        self.api_url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "    def get_weather(self, location: str) -> str:\n",
    "        params = {\n",
    "            \"q\": location,\n",
    "            \"appid\": self.api_key,\n",
    "            \"units\": \"metric\"  # return ¬∞C instead of Kelvin\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(self.api_url, params=params)\n",
    "            data = response.json()\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                return f\"Error: {data.get('message', 'Unable to fetch weather')}\"\n",
    "\n",
    "            # Parse relevant info\n",
    "            city = data.get(\"name\", location)\n",
    "            country = data.get(\"sys\", {}).get(\"country\", \"\")\n",
    "            weather_main = data[\"weather\"][0][\"main\"]\n",
    "            weather_desc = data[\"weather\"][0][\"description\"]\n",
    "            temp = data[\"main\"][\"temp\"]\n",
    "            feels_like = data[\"main\"][\"feels_like\"]\n",
    "            humidity = data[\"main\"][\"humidity\"]\n",
    "            wind_speed = data[\"wind\"][\"speed\"]\n",
    "\n",
    "            return (\n",
    "                f\"Weather in {city}, {country}:\\n\"\n",
    "                f\"- Condition: {weather_main} ({weather_desc})\\n\"\n",
    "                f\"- Temperature: {temp:.1f}¬∞C (feels like {feels_like:.1f}¬∞C)\\n\"\n",
    "                f\"- Humidity: {humidity}%\\n\"\n",
    "                f\"- Wind speed: {wind_speed} m/s\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return f\"Error fetching weather: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa1075ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "class AIServiceWithTools(AIService):\n",
    "    model_gen_image = 'gemini-2.0-flash-preview-image-generation'\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tools = tools\n",
    "        self.client_gemini = genai.Client(\n",
    "            api_key=env_service.get_gemini_api_key(),\n",
    "        )\n",
    "    def chat_with_tools(self, messages):\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            tools=self.tools,\n",
    "            stream=True\n",
    "        )\n",
    "        return responses\n",
    "    def generate_image(self, prompt, save_path):\n",
    "        response = self.client_gemini.models.generate_content(\n",
    "            model=\"gemini-2.0-flash-preview-image-generation\",\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "            response_modalities=['TEXT', 'IMAGE']\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for part in response.candidates[0].content.parts:\n",
    "            if part.text is not None:\n",
    "                print(part.text)\n",
    "            elif part.inline_data is not None:\n",
    "                image = Image.open(BytesIO((part.inline_data.data)))\n",
    "                image.save(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b44935a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import gradio as gr\n",
    "\n",
    "class ChatBotWithTools(ChatBot):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ai_service = AIServiceWithTools()\n",
    "        self.weather_service = WeatherService()\n",
    "\n",
    "    # --- Utility: clean history before sending to model ---\n",
    "    def sanitize_history_for_model(self, history):\n",
    "        cleaned = []\n",
    "        for msg in history:\n",
    "            role = msg.get(\"role\")\n",
    "            content = msg.get(\"content\")\n",
    "\n",
    "            # Skip image dicts from Gradio\n",
    "            if isinstance(content, dict):\n",
    "                img_desc = content.get(\"path\", \"\")\n",
    "                if img_desc:\n",
    "                    cleaned.append({\n",
    "                        \"role\": role,\n",
    "                        \"content\": f\"[Image shown to user: {img_desc}]\"\n",
    "                    })\n",
    "                continue\n",
    "\n",
    "            # Extract text from multimodal lists if any\n",
    "            if isinstance(content, list):\n",
    "                text_parts = []\n",
    "                for c in content:\n",
    "                    if isinstance(c, dict) and c.get(\"type\") == \"text\":\n",
    "                        text_parts.append(c.get(\"text\", \"\"))\n",
    "                    elif isinstance(c, str):\n",
    "                        text_parts.append(c)\n",
    "                content = \"\\n\".join(text_parts)\n",
    "\n",
    "            # Convert tuples to string\n",
    "            if isinstance(content, tuple):\n",
    "                content = \" \".join(map(str, content))\n",
    "\n",
    "            # Skip empty messages\n",
    "            if not content or not isinstance(content, str):\n",
    "                continue\n",
    "\n",
    "            cleaned.append({\"role\": role, \"content\": content})\n",
    "        return cleaned\n",
    "\n",
    "\n",
    "    # --- Main chat handler ---\n",
    "    def chat_with_tools(self, messages, history):\n",
    "        # Step 1: sanitize previous conversation before sending to model\n",
    "        safe_history = self.sanitize_history_for_model(history)\n",
    "\n",
    "        # Step 2: append current user message (handle multimodal format from Gradio)\n",
    "        user_content = messages\n",
    "        if isinstance(messages, list):\n",
    "            # Extract text from multimodal message format\n",
    "            text_parts = [m.get(\"text\", \"\") for m in messages if isinstance(m, dict) and m.get(\"type\") == \"text\"]\n",
    "            user_content = \"\\n\".join(text_parts) if text_parts else messages\n",
    "        elif isinstance(messages, dict):\n",
    "            # Handle dict format (e.g., file uploads)\n",
    "            user_content = str(messages)\n",
    "        \n",
    "        new_messages = copy.deepcopy(safe_history)\n",
    "        new_messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "        # Step 3: stream model responses\n",
    "        print(\"Sending to model...\", new_messages)\n",
    "        responses = self.ai_service.chat_with_tools(new_messages)\n",
    "\n",
    "        partial = \"\"\n",
    "        tool_call_data = {}\n",
    "\n",
    "        for chunk in responses:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            # --- Stream normal text ---\n",
    "            if delta.content is not None:\n",
    "                partial += delta.content\n",
    "                yield [{\"role\": \"assistant\", \"content\": partial}]\n",
    "\n",
    "            # --- Collect streamed tool calls ---\n",
    "            if delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls:\n",
    "                    idx = tool_call.index\n",
    "                    fn_name = tool_call.function.name\n",
    "                    fn_args_part = tool_call.function.arguments\n",
    "\n",
    "                    if idx not in tool_call_data:\n",
    "                        tool_call_data[idx] = {\"name\": fn_name, \"args\": \"\"}\n",
    "\n",
    "                    if fn_name:\n",
    "                        tool_call_data[idx][\"name\"] = fn_name\n",
    "                    if fn_args_part:\n",
    "                        tool_call_data[idx][\"args\"] += fn_args_part\n",
    "\n",
    "        # Step 4: Execute tools after streaming completes\n",
    "        for idx, tool in tool_call_data.items():\n",
    "            fn_name = tool[\"name\"]\n",
    "            args_str = tool[\"args\"]\n",
    "\n",
    "            try:\n",
    "                args = json.loads(args_str)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to parse tool args:\", args_str, e)\n",
    "                args = {}\n",
    "\n",
    "            # --- Weather tool ---\n",
    "            if fn_name == \"get_weather\":\n",
    "                tool_result = self.weather_service.get_weather(**args)\n",
    "                yield [{\"role\": \"assistant\", \"content\": tool_result}]\n",
    "\n",
    "            # --- Image generation tool ---\n",
    "            elif fn_name == \"generate_image\":\n",
    "                prompt = args.get(\"prompt\", \"\")\n",
    "                save_path = \"generate_image.png\"\n",
    "                self.ai_service.generate_image(prompt, save_path)\n",
    "\n",
    "                # Step 4a: Tell user what image was made\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Here is your image for: {prompt}\"}]\n",
    "\n",
    "                # Step 4b: Show image in Gradio\n",
    "                yield [{\"role\": \"assistant\", \"content\": {\"path\": save_path, \"mime_type\": \"image/png\"}}]\n",
    "\n",
    "                # Step 4c: Add text reference so model can remember next time\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"[Generated image for: {prompt}]\"})\n",
    "\n",
    "            else:\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Unknown function call: {fn_name}\"}]\n",
    "\n",
    "    # --- Gradio UI setup ---\n",
    "    def render_ui(self):\n",
    "        chat_interface = gr.ChatInterface(fn=self.chat_with_tools, type=\"messages\")\n",
    "        chat_interface.launch()\n",
    "\n",
    "    def run(self):\n",
    "        self.render_ui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a489c8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_bot_with_tools = ChatBotWithTools()\n",
    "chat_bot_with_tools.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a8645",
   "metadata": {},
   "source": [
    "## ChatBot with MCP\n",
    "\n",
    "\n",
    "* Extending the base ChatBot class to support tool usage via MCP\n",
    "* Below example shows how to interact between chat bot and tools using MCP protocol. MCP google calendar server is used to manage calendar events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9e779dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIServiceWithMCP(AIServiceWithTools):\n",
    "    def chat_with_mcp(self, messages):\n",
    "        responses = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            stream=True\n",
    "        )\n",
    "        return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6edb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Optional\n",
    "from contextlib import AsyncExitStack\n",
    "import nest_asyncio\n",
    "\n",
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "# Allow nested event loops (required for Jupyter + Gradio + MCP)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class ChatBotWithMCP(ChatBotWithTools):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mcp_session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.mcp_tools = []\n",
    "        self.ai_service_with_mcp = AIServiceWithMCP()\n",
    "        self.event_loop = None  # Store the loop where MCP session was created\n",
    "    \n",
    "    async def start_mcp_session(self):\n",
    "        \"\"\"Connect to SSE-based MCP server and keep session alive\"\"\"\n",
    "        server_url = \"http://localhost:8080/sse\"\n",
    "        print(\"Connecting to MCP server at:\", server_url)\n",
    "        # Store the event loop where we create the session\n",
    "        self.event_loop = asyncio.get_event_loop()\n",
    "        \n",
    "        # Keep the context alive by storing in exit_stack\n",
    "        read, write = await self.exit_stack.enter_async_context(sse_client(server_url))\n",
    "        print(\"SSE client connected to MCP server\")\n",
    "        self.mcp_session = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
    "        print(\"MCP session created\")\n",
    "        \n",
    "        await self.mcp_session.initialize()\n",
    "        \n",
    "        # List available tools\n",
    "        response = await self.mcp_session.list_tools()\n",
    "        print(\"\\nConnected to MCP server with tools:\", [tool.name for tool in response.tools])\n",
    "        self.mcp_tools = response.tools\n",
    "        \n",
    "        return self.mcp_session\n",
    "\n",
    "    async def start_mcp_session_stdio(self):\n",
    "        \"\"\"Connect to MCP server using stdio transport\"\"\"\n",
    "        from mcp.client.stdio import stdio_client, StdioServerParameters\n",
    "        from pathlib import Path\n",
    "        \n",
    "        print(\"Connecting to MCP server via stdio transport\")\n",
    "        # Store the event loop where we create the session\n",
    "        self.event_loop = asyncio.get_event_loop()\n",
    "        \n",
    "        # Determine the correct path to MCP server\n",
    "        current_path = Path.cwd()\n",
    "        if current_path.name == 'src':\n",
    "            mcp_server_path = current_path.parent / \"google-calendar-mcp\" / \"build\" / \"index.js\"\n",
    "        else:\n",
    "            mcp_server_path = current_path / \"google-calendar-mcp\" / \"build\" / \"index.js\"\n",
    "        \n",
    "        # Check if server file exists\n",
    "        if not mcp_server_path.exists():\n",
    "            error_msg = (\n",
    "                f\"‚ùå MCP server not found at: {mcp_server_path}\\n\"\n",
    "                f\"Current directory: {Path.cwd()}\\n\"\n",
    "                f\"Please build your MCP server first by running:\\n\"\n",
    "                f\"  cd google-calendar-mcp && npm install && npm run build\"\n",
    "            )\n",
    "            print(error_msg)\n",
    "            raise FileNotFoundError(error_msg)\n",
    "        \n",
    "        # Configure the stdio client with your MCP server command\n",
    "        server_params = StdioServerParameters(\n",
    "            command=\"node\",  # or \"python\" depending on your server\n",
    "            args=[str(mcp_server_path)],  # absolute path to your MCP server\n",
    "            env=None  # Add environment variables if needed\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì Found MCP server at: {mcp_server_path}\")\n",
    "        print(f\"Starting MCP server with command: {server_params.command} {' '.join(server_params.args)}\")\n",
    "        \n",
    "        try:\n",
    "            # Keep the context alive by storing in exit_stack\n",
    "            read, write = await self.exit_stack.enter_async_context(\n",
    "                stdio_client(server_params)\n",
    "            )\n",
    "            print(\"‚úì Stdio client connected to MCP server\")\n",
    "            \n",
    "            self.mcp_session = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
    "            print(\"‚úì MCP session created via stdio\")\n",
    "            \n",
    "            await self.mcp_session.initialize()\n",
    "            print(\"‚úì MCP session initialized\")\n",
    "            \n",
    "            # List available tools\n",
    "            response = await self.mcp_session.list_tools()\n",
    "            print(f\"\\n‚úì Connected to MCP server with {len(response.tools)} tools\")\n",
    "            print(\"üìã Available MCP Tools:\")\n",
    "            for tool in response.tools:\n",
    "                print(f\"   - {tool.name}: {tool.description if hasattr(tool, 'description') else 'No description'}\")\n",
    "            self.mcp_tools = response.tools\n",
    "            \n",
    "            return self.mcp_session\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = (\n",
    "                f\"‚ùå Failed to connect to MCP server:\\n\"\n",
    "                f\"Error: {str(e)}\\n\\n\"\n",
    "                f\"Troubleshooting tips:\\n\"\n",
    "                f\"1. Make sure Node.js is installed: run 'node --version'\\n\"\n",
    "                f\"2. Build the MCP server: cd google-calendar-mcp && npm run build\\n\"\n",
    "                f\"3. Test the server manually: node {mcp_server_path}\\n\"\n",
    "                f\"4. Check server logs for any startup errors\\n\"\n",
    "            )\n",
    "            print(error_msg)\n",
    "            raise\n",
    "\n",
    "    def convert_mcp_tools_to_openai_format(self):\n",
    "        \"\"\"Convert MCP tools to OpenAI function calling format.\"\"\"\n",
    "        print(f\"\\nüîÑ Converting {len(self.mcp_tools)} MCP tools to OpenAI format...\")\n",
    "        openai_tools = []\n",
    "        for tool in self.mcp_tools:\n",
    "            # Get input schema or create default\n",
    "            input_schema = tool.inputSchema if hasattr(tool, 'inputSchema') and tool.inputSchema else {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {},\n",
    "                \"required\": []\n",
    "            }\n",
    "            \n",
    "            # Ensure input_schema has required fields\n",
    "            if not isinstance(input_schema, dict):\n",
    "                input_schema = {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "            if \"type\" not in input_schema:\n",
    "                input_schema[\"type\"] = \"object\"\n",
    "            if \"properties\" not in input_schema:\n",
    "                input_schema[\"properties\"] = {}\n",
    "            \n",
    "            openai_tool = {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool.name,\n",
    "                    \"description\": tool.description if hasattr(tool, 'description') and tool.description else f\"Execute {tool.name}\",\n",
    "                    \"parameters\": input_schema\n",
    "                }\n",
    "            }\n",
    "            openai_tools.append(openai_tool)\n",
    "            \n",
    "        print(f\"Converted {len(openai_tools)} MCP tools to OpenAI format\")\n",
    "        # Merge with existing tools (weather, image generation)\n",
    "        all_tools = tools + openai_tools  # Combine built-in and MCP tools\n",
    "        print(f\"Total tools available: {len(all_tools)} (Built-in: {len(tools)}, MCP: {len(openai_tools)})\")\n",
    "        return all_tools\n",
    "\n",
    "    async def execute_mcp_tool(self, tool_name: str, arguments: dict) -> str:\n",
    "        \"\"\"Execute a tool on the MCP server.\"\"\"\n",
    "        if not self.mcp_session:\n",
    "            return \"Error: MCP session not initialized\"\n",
    "        \n",
    "        try:\n",
    "            print(f\"Executing MCP tool: {tool_name} with args: {arguments}\")\n",
    "            result = await self.mcp_session.call_tool(tool_name, arguments=arguments)\n",
    "            print(f\"MCP tool result: {result}\")\n",
    "            \n",
    "            # Extract text content from result\n",
    "            if hasattr(result, 'content') and len(result.content) > 0:\n",
    "                content_item = result.content[0]\n",
    "                if hasattr(content_item, 'text'):\n",
    "                    return content_item.text\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            error_details = traceback.format_exc()\n",
    "            print(f\"Error executing MCP tool: {error_details}\")\n",
    "            return f\"Error executing tool {tool_name}: {str(e)}\"\n",
    "\n",
    "    def format_mcp_response(self, tool_name: str, raw_result: str, arguments: dict) -> str:\n",
    "        \"\"\"\n",
    "        Format raw MCP tool results into user-friendly messages.\n",
    "        \n",
    "        Args:\n",
    "            tool_name: Name of the MCP tool that was executed\n",
    "            raw_result: Raw result from the MCP server\n",
    "            arguments: Arguments passed to the tool\n",
    "            \n",
    "        Returns:\n",
    "            Formatted, user-friendly response string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try to parse as JSON for structured responses\n",
    "            try:\n",
    "                data = json.loads(raw_result)\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                data = None\n",
    "            \n",
    "            # Calendar-specific formatting\n",
    "            if tool_name == \"list_calendars\":\n",
    "                if data and isinstance(data, dict):\n",
    "                    calendars = data.get('calendars', [])\n",
    "                    if calendars:\n",
    "                        response = \"üìÖ **Your Calendars:**\\n\\n\"\n",
    "                        for cal in calendars:\n",
    "                            response += f\"‚Ä¢ **{cal.get('summary', 'Unnamed')}**\\n\"\n",
    "                            if cal.get('description'):\n",
    "                                response += f\"  _{cal.get('description')}_\\n\"\n",
    "                        return response\n",
    "                    return \"You don't have any calendars yet.\"\n",
    "                \n",
    "            elif tool_name == \"list_events\":\n",
    "                if data and isinstance(data, dict):\n",
    "                    events = data.get('events', [])\n",
    "                    if events:\n",
    "                        response = \"üìã **Upcoming Events:**\\n\\n\"\n",
    "                        for event in events:\n",
    "                            summary = event.get('summary', 'Untitled Event')\n",
    "                            start = event.get('start', {}).get('dateTime', 'No time')\n",
    "                            response += f\"‚Ä¢ **{summary}**\\n  üïê {start}\\n\"\n",
    "                            if event.get('location'):\n",
    "                                response += f\"  üìç {event.get('location')}\\n\"\n",
    "                        return response\n",
    "                    return \"No upcoming events found.\"\n",
    "                    \n",
    "            elif tool_name == \"create_event\":\n",
    "                if data and isinstance(data, dict):\n",
    "                    event = data.get('event', {})\n",
    "                    summary = event.get('summary', arguments.get('summary', 'Event'))\n",
    "                    start = event.get('start', {}).get('dateTime', '')\n",
    "                    return f\"‚úÖ **Event Created Successfully!**\\n\\nüìå {summary}\\nüïê {start}\"\n",
    "                elif \"successfully\" in raw_result.lower() or \"created\" in raw_result.lower():\n",
    "                    return f\"‚úÖ **Event Created Successfully!**\\n\\nüìå {arguments.get('summary', 'Your event')}\"\n",
    "                    \n",
    "            elif tool_name == \"update_event\":\n",
    "                if \"successfully\" in raw_result.lower() or \"updated\" in raw_result.lower():\n",
    "                    return f\"‚úÖ **Event Updated Successfully!**\"\n",
    "                    \n",
    "            elif tool_name == \"delete_event\":\n",
    "                if \"successfully\" in raw_result.lower() or \"deleted\" in raw_result.lower():\n",
    "                    return f\"üóëÔ∏è **Event Deleted Successfully!**\"\n",
    "                    \n",
    "            elif tool_name == \"search_events\":\n",
    "                if data and isinstance(data, dict):\n",
    "                    events = data.get('events', [])\n",
    "                    query = arguments.get('query', '')\n",
    "                    if events:\n",
    "                        response = f\"üîç **Search Results for \\\"{query}\\\":**\\n\\n\"\n",
    "                        for event in events:\n",
    "                            summary = event.get('summary', 'Untitled Event')\n",
    "                            start = event.get('start', {}).get('dateTime', 'No time')\n",
    "                            response += f\"‚Ä¢ **{summary}**\\n  üïê {start}\\n\"\n",
    "                        return response\n",
    "                    return f\"No events found matching \\\"{query}\\\".\"\n",
    "                    \n",
    "            elif tool_name == \"get_current_time\":\n",
    "                if data and isinstance(data, dict):\n",
    "                    time = data.get('time', '')\n",
    "                    timezone = data.get('timezone', '')\n",
    "                    return f\"üïê Current time: **{time}** ({timezone})\"\n",
    "            \n",
    "            # Generic fallback: If it looks like an error\n",
    "            if \"error\" in raw_result.lower() or \"failed\" in raw_result.lower():\n",
    "                return f\"‚ùå {raw_result}\"\n",
    "            \n",
    "            # If we have structured data but no specific formatter\n",
    "            if data:\n",
    "                # Try to make it more readable\n",
    "                formatted = json.dumps(data, indent=2)\n",
    "                return f\"‚ÑπÔ∏è **Result:**\\n```\\n{formatted}\\n```\"\n",
    "            \n",
    "            # Return raw result if nothing else matches, but clean it up\n",
    "            return raw_result.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error formatting MCP response: {e}\")\n",
    "            return raw_result  # Fall back to raw result if formatting fails\n",
    "\n",
    "    def chat_with_mcp_tool(self, messages, history):\n",
    "        \"\"\"Chat method that uses MCP server tools with OpenAI function calling.\"\"\"\n",
    "        # Sanitize history\n",
    "        safe_history = self.sanitize_history_for_model(history)\n",
    "        print('Chatting with mcp tools')\n",
    "        # Prepare user message\n",
    "        user_content = messages\n",
    "        if isinstance(messages, list):\n",
    "            text_parts = [m.get(\"text\", \"\") for m in messages if isinstance(m, dict) and m.get(\"type\") == \"text\"]\n",
    "            user_content = \"\\n\".join(text_parts) if text_parts else messages\n",
    "        elif isinstance(messages, dict):\n",
    "            user_content = str(messages)\n",
    "        \n",
    "        new_messages = copy.deepcopy(safe_history)\n",
    "        new_messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "        # Get all tools (MCP + built-in)\n",
    "        all_tools = self.convert_mcp_tools_to_openai_format()\n",
    "        \n",
    "        # Call OpenAI with combined tools\n",
    "        responses = self.ai_service.client.chat.completions.create(\n",
    "            model=self.ai_service.model,\n",
    "            messages=new_messages,\n",
    "            tools=all_tools,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        partial = \"\"\n",
    "        tool_call_data = {}\n",
    "\n",
    "        for chunk in responses:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            # Stream text content\n",
    "            if delta.content is not None:\n",
    "                partial += delta.content\n",
    "                yield [{\"role\": \"assistant\", \"content\": partial}]\n",
    "\n",
    "            # Collect tool calls\n",
    "            if delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls:\n",
    "                    idx = tool_call.index\n",
    "                    fn_name = tool_call.function.name\n",
    "                    fn_args_part = tool_call.function.arguments\n",
    "\n",
    "                    if idx not in tool_call_data:\n",
    "                        tool_call_data[idx] = {\"name\": fn_name, \"args\": \"\"}\n",
    "\n",
    "                    if fn_name:\n",
    "                        tool_call_data[idx][\"name\"] = fn_name\n",
    "                    if fn_args_part:\n",
    "                        tool_call_data[idx][\"args\"] += fn_args_part\n",
    "\n",
    "        # Execute collected tool calls\n",
    "        if tool_call_data:\n",
    "            print(f\"\\nüîß Tool calls detected: {len(tool_call_data)} tool(s)\")\n",
    "        \n",
    "        for idx, tool in tool_call_data.items():\n",
    "            fn_name = tool[\"name\"]\n",
    "            args_str = tool[\"args\"]\n",
    "\n",
    "            try:\n",
    "                args = json.loads(args_str)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to parse tool args:\", args_str, e)\n",
    "                args = {}\n",
    "\n",
    "            # Check if it's an MCP tool\n",
    "            mcp_tool_names = [t.name for t in self.mcp_tools]\n",
    "            if fn_name in mcp_tool_names:\n",
    "                print(f\"üîå MCP Tool detected: '{fn_name}'\")\n",
    "                print(f\"   Arguments: {json.dumps(args, indent=2)}\")\n",
    "                # Execute MCP tool using the same event loop where session was created\n",
    "                try:\n",
    "                    if self.event_loop and self.event_loop.is_running():\n",
    "                        # Use the same loop that created the MCP session\n",
    "                        future = asyncio.run_coroutine_threadsafe(\n",
    "                            self.execute_mcp_tool(fn_name, args),\n",
    "                            self.event_loop\n",
    "                        )\n",
    "                        raw_result = future.result(timeout=30)\n",
    "                    else:\n",
    "                        # Fallback if loop is not running\n",
    "                        raw_result = asyncio.run(self.execute_mcp_tool(fn_name, args))\n",
    "                    \n",
    "                    # Ensure we got a valid result\n",
    "                    if raw_result is None:\n",
    "                        raw_result = \"No response from MCP tool\"\n",
    "                    else:\n",
    "                        print(f\"‚úÖ MCP Tool '{fn_name}' executed successfully\")\n",
    "                    \n",
    "                    # Format the response for better user experience\n",
    "                    formatted_result = self.format_mcp_response(fn_name, raw_result, args)\n",
    "                    print(f\"üìù Formatted response: {formatted_result[:100]}...\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    import traceback\n",
    "                    print(f\"‚ùå MCP Tool '{fn_name}' failed: {str(e)}\")\n",
    "                    formatted_result = f\"‚ùå Sorry, I encountered an error: {str(e)}\"\n",
    "                    \n",
    "                yield [{\"role\": \"assistant\", \"content\": formatted_result}]\n",
    "            \n",
    "            # Handle built-in tools\n",
    "            elif fn_name == \"get_weather\":\n",
    "                print(f\"üå§Ô∏è  Built-in Tool: 'get_weather'\")\n",
    "                print(f\"   Arguments: {json.dumps(args, indent=2)}\")\n",
    "                tool_result = self.weather_service.get_weather(**args)\n",
    "                yield [{\"role\": \"assistant\", \"content\": tool_result}]\n",
    "\n",
    "            elif fn_name == \"generate_image\":\n",
    "                print(f\"üé® Built-in Tool: 'generate_image'\")\n",
    "                print(f\"   Arguments: {json.dumps(args, indent=2)}\")\n",
    "                prompt = args.get(\"prompt\", \"\")\n",
    "                save_path = \"generate_image.png\"\n",
    "                self.ai_service.generate_image(prompt, save_path)\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Here is your image for: {prompt}\"}]\n",
    "                yield [{\"role\": \"assistant\", \"content\": {\"path\": save_path, \"mime_type\": \"image/png\"}}]\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"[Generated image for: {prompt}]\"})\n",
    "\n",
    "            else:\n",
    "                print(f\"‚ùå Unknown Tool: '{fn_name}'\")\n",
    "                print(f\"   Arguments: {json.dumps(args, indent=2)}\")\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Unknown function call: {fn_name}\"}]\n",
    "\n",
    "    # --- Gradio UI setup ---\n",
    "    def render_ui_with_mcp(self):\n",
    "        \"\"\"Render UI that uses MCP tools.\"\"\"\n",
    "        chat_interface = gr.ChatInterface(fn=self.chat_with_mcp_tool, type=\"messages\")\n",
    "        chat_interface.launch()\n",
    "\n",
    "    async def run_with_mcp_async(self):\n",
    "        \"\"\"Async wrapper to initialize MCP before launching UI\"\"\"\n",
    "        await self.start_mcp_session_stdio()\n",
    "        self.render_ui_with_mcp()\n",
    "\n",
    "    def run_with_mcp(self):\n",
    "        \"\"\"Run chatbot with MCP tools.\"\"\"\n",
    "        # Initialize MCP session before starting UI\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # Jupyter notebook case - create a task and let nest_asyncio handle it\n",
    "            import nest_asyncio\n",
    "            nest_asyncio.apply()\n",
    "            # Use asyncio.create_task to properly schedule the coroutine\n",
    "            task = asyncio.create_task(self.run_with_mcp_async())\n",
    "            # Wait for it to complete in the notebook environment\n",
    "            loop.run_until_complete(task)\n",
    "        else:\n",
    "            loop.run_until_complete(self.run_with_mcp_async())\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Cleanup MCP session when done\"\"\"\n",
    "        await self.exit_stack.aclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f61c854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MCP server via stdio transport\n",
      "‚úì Found MCP server at: /root/backend/research/ai-chatbot/google-calendar-mcp/build/index.js\n",
      "Starting MCP server with command: node /root/backend/research/ai-chatbot/google-calendar-mcp/build/index.js\n",
      "‚úì Stdio client connected to MCP server\n",
      "‚úì MCP session created via stdio\n",
      "‚úì MCP session initialized\n",
      "\n",
      "‚úì Connected to MCP server with 10 tools\n",
      "üìã Available MCP Tools:\n",
      "   - list-calendars: List all available calendars\n",
      "   - list-events: List events from one or more calendars. Supports both calendar IDs and calendar names.\n",
      "   - search-events: Search for events in a calendar by text query.\n",
      "   - get-event: Get details of a specific event by ID.\n",
      "   - list-colors: List available color IDs and their meanings for calendar events\n",
      "   - create-event: Create a new calendar event.\n",
      "   - update-event: Update an existing calendar event with recurring event modification scope support.\n",
      "   - delete-event: Delete a calendar event.\n",
      "   - get-freebusy: Query free/busy information for calendars. Note: Time range is limited to a maximum of 3 months between timeMin and timeMax.\n",
      "   - get-current-time: Get current time in the primary Google Calendar's timezone (or a requested timezone).\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and test MCP connection\n",
    "chat_bot_with_mcp = ChatBotWithMCP()\n",
    "chat_bot_with_mcp.run_with_mcp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758df73",
   "metadata": {},
   "source": [
    "# ChatBot with RAG\n",
    "\n",
    "* Using Retrieval-Augmented Generation (RAG) to enhance chatbot responses with relevant external information.\n",
    "* Below example shows how to integrate a document retrieval system (in knowledge base folder) with a chatbot to provide more accurate and context-aware answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f00e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to path for importing embedded module\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'src':\n",
    "    sys.path.insert(0, str(current_dir))\n",
    "else:\n",
    "    src_dir = current_dir / 'src'\n",
    "    if src_dir.exists():\n",
    "        sys.path.insert(0, str(src_dir))\n",
    "\n",
    "from embedded import KnowledgeBaseVectorizer\n",
    "\n",
    "class ChatBotWithRAG(ChatBotWithMCP):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize RAG-specific components with pgvector\n",
    "        self.vectorizer = self._init_vectorizer()\n",
    "    \n",
    "    def _init_vectorizer(self):\n",
    "        \"\"\"Initialize the KnowledgeBaseVectorizer for RAG retrieval\"\"\"\n",
    "        db_config = {\n",
    "            'host': os.getenv('DB_HOST', 'localhost'),\n",
    "            'port': os.getenv('DB_PORT', '5432'),\n",
    "            'database': os.getenv('DB_NAME', 'ai_chatbot'),\n",
    "            'user': os.getenv('DB_USER', 'postgres'),\n",
    "            'password': os.getenv('DB_PASSWORD', 'postgres')\n",
    "        }\n",
    "        \n",
    "        # Path to knowledge base - navigate from current directory\n",
    "        current_path = Path.cwd()\n",
    "        if current_path.name == 'src':\n",
    "            knowledge_base_path = current_path.parent / 'knowledge-base'\n",
    "        else:\n",
    "            knowledge_base_path = current_path / 'knowledge-base'\n",
    "        \n",
    "        vectorizer = KnowledgeBaseVectorizer(str(knowledge_base_path), db_config)\n",
    "        vectorizer.connect_db()\n",
    "        print(\"‚úì RAG vectorizer initialized and connected to database\")\n",
    "        \n",
    "        return vectorizer\n",
    "    \n",
    "    def retrieve_relevant_docs(self, query: str, top_k: int = 3) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from pgvector based on the query\n",
    "        \n",
    "        Args:\n",
    "            query: User's question/query\n",
    "            top_k: Number of relevant documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of relevant document contents\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Use pgvector similarity search to find relevant content\n",
    "            results = self.vectorizer.search_similar(\n",
    "                query=query,\n",
    "                top_k=top_k,\n",
    "                expand_query=True,  # Use intelligent query expansion\n",
    "                min_similarity=0.3   # Only return reasonably relevant results\n",
    "            )\n",
    "            \n",
    "            # Extract content from results and clean metadata prefixes\n",
    "            relevant_docs = []\n",
    "            for result in results:\n",
    "                content = result['content']\n",
    "                # Remove metadata prefix for cleaner context\n",
    "                if '\\n\\n' in content:\n",
    "                    content = content.split('\\n\\n', 1)[-1]\n",
    "                \n",
    "                # Add source information\n",
    "                source_info = f\"[Source: {result['file_path']} - Similarity: {result['similarity']:.2f}]\"\n",
    "                relevant_docs.append(f\"{source_info}\\n{content}\")\n",
    "            \n",
    "            return relevant_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving documents: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def chat_with_rag(self, messages, history):\n",
    "        # Step 1: Sanitize history\n",
    "        safe_history = self.sanitize_history_for_model(history)\n",
    "        \n",
    "        # Step 2: Prepare user message\n",
    "        user_content = messages\n",
    "        if isinstance(messages, list):\n",
    "            text_parts = [m.get(\"text\", \"\") for m in messages if isinstance(m, dict) and m.get(\"type\") == \"text\"]\n",
    "            user_content = \"\\n\".join(text_parts) if text_parts else messages\n",
    "        elif isinstance(messages, dict):\n",
    "            user_content = str(messages)\n",
    "        \n",
    "        new_messages = copy.deepcopy(safe_history)\n",
    "        \n",
    "        # Step 3: Retrieve relevant documents from pgvector\n",
    "        print(f\"üîç Retrieving relevant documents for: {user_content}\")\n",
    "        relevant_docs = self.retrieve_relevant_docs(user_content, top_k=3)\n",
    "        \n",
    "        # Step 4: Augment user message with retrieved docs if found\n",
    "        if relevant_docs:\n",
    "            docs_content = \"\\n\\n---\\n\\n\".join(relevant_docs)\n",
    "            augmented_message = (\n",
    "                f\"User Question: {user_content}\\n\\n\"\n",
    "                f\"Relevant Context from Knowledge Base:\\n{docs_content}\\n\\n\"\n",
    "                f\"Please answer the user's question using the provided context. \"\n",
    "                f\"If the context is relevant, use it to provide accurate information. \"\n",
    "                f\"If the context is not relevant, answer based on your general knowledge.\"\n",
    "            )\n",
    "            print(f\"‚úì Found {len(relevant_docs)} relevant documents\")\n",
    "        else:\n",
    "            augmented_message = user_content\n",
    "            print(\"‚ö†Ô∏è  No relevant documents found, using general knowledge\")\n",
    "        \n",
    "        new_messages.append({\"role\": \"user\", \"content\": augmented_message})\n",
    "        \n",
    "        # Step 5: Get all tools (MCP + built-in)\n",
    "        all_tools = self.convert_mcp_tools_to_openai_format() if self.mcp_tools else tools\n",
    "        \n",
    "        # Step 6: Call chat with tools\n",
    "        responses = self.ai_service.client.chat.completions.create(\n",
    "            model=self.ai_service.model,\n",
    "            messages=new_messages,\n",
    "            tools=all_tools,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        partial = \"\"\n",
    "        tool_call_data = {}\n",
    "\n",
    "        for chunk in responses:\n",
    "            delta = chunk.choices[0].delta\n",
    "\n",
    "            # Stream text content\n",
    "            if delta.content is not None:\n",
    "                partial += delta.content\n",
    "                yield [{\"role\": \"assistant\", \"content\": partial}]\n",
    "\n",
    "            # Collect tool calls\n",
    "            if delta.tool_calls:\n",
    "                for tool_call in delta.tool_calls:\n",
    "                    idx = tool_call.index\n",
    "                    fn_name = tool_call.function.name\n",
    "                    fn_args_part = tool_call.function.arguments\n",
    "\n",
    "                    if idx not in tool_call_data:\n",
    "                        tool_call_data[idx] = {\"name\": fn_name, \"args\": \"\"}\n",
    "\n",
    "                    if fn_name:\n",
    "                        tool_call_data[idx][\"name\"] = fn_name\n",
    "                    if fn_args_part:\n",
    "                        tool_call_data[idx][\"args\"] += fn_args_part\n",
    "\n",
    "        # Execute collected tool calls\n",
    "        for idx, tool in tool_call_data.items():\n",
    "            fn_name = tool[\"name\"]\n",
    "            args_str = tool[\"args\"]\n",
    "\n",
    "            try:\n",
    "                args = json.loads(args_str)\n",
    "            except Exception as e:\n",
    "                print(\"Failed to parse tool args:\", args_str, e)\n",
    "                args = {}\n",
    "\n",
    "            # Check if it's an MCP tool\n",
    "            if self.mcp_tools:\n",
    "                mcp_tool_names = [t.name for t in self.mcp_tools]\n",
    "                if fn_name in mcp_tool_names:\n",
    "                    try:\n",
    "                        if self.event_loop and self.event_loop.is_running():\n",
    "                            future = asyncio.run_coroutine_threadsafe(\n",
    "                                self.execute_mcp_tool(fn_name, args),\n",
    "                                self.event_loop\n",
    "                            )\n",
    "                            tool_result = future.result(timeout=30)\n",
    "                        else:\n",
    "                            tool_result = asyncio.run(self.execute_mcp_tool(fn_name, args))\n",
    "                        \n",
    "                        if tool_result is None:\n",
    "                            tool_result = \"No response from MCP tool\"\n",
    "                    except Exception as e:\n",
    "                        import traceback\n",
    "                        tool_result = f\"Error executing MCP tool: {str(e)}\\n{traceback.format_exc()}\"\n",
    "                    \n",
    "                    yield [{\"role\": \"assistant\", \"content\": tool_result}]\n",
    "                    continue\n",
    "            \n",
    "            # Handle built-in tools\n",
    "            if fn_name == \"get_weather\":\n",
    "                tool_result = self.weather_service.get_weather(**args)\n",
    "                yield [{\"role\": \"assistant\", \"content\": tool_result}]\n",
    "\n",
    "            elif fn_name == \"generate_image\":\n",
    "                prompt = args.get(\"prompt\", \"\")\n",
    "                save_path = \"generate_image.png\"\n",
    "                self.ai_service.generate_image(prompt, save_path)\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Here is your image for: {prompt}\"}]\n",
    "                yield [{\"role\": \"assistant\", \"content\": {\"path\": save_path, \"mime_type\": \"image/png\"}}]\n",
    "                history.append({\"role\": \"assistant\", \"content\": f\"[Generated image for: {prompt}]\"})\n",
    "\n",
    "            else:\n",
    "                yield [{\"role\": \"assistant\", \"content\": f\"Unknown function call: {fn_name}\"}]\n",
    "    \n",
    "    def render_ui_with_rag(self):\n",
    "        \"\"\"Render UI that uses RAG with pgvector.\"\"\"\n",
    "        chat_interface = gr.ChatInterface(\n",
    "            fn=self.chat_with_rag, \n",
    "            type=\"messages\",\n",
    "            title=\"AI Chatbot with RAG (pgvector)\",\n",
    "            description=\"Ask questions and I'll search the knowledge base for relevant information!\"\n",
    "        )\n",
    "        chat_interface.launch()\n",
    "\n",
    "    async def run_with_rag_async(self):\n",
    "        \"\"\"Async wrapper to initialize MCP (if available) before launching UI\"\"\"\n",
    "        try:\n",
    "            await self.start_mcp_session()\n",
    "            print(\"‚úì MCP session started\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  MCP not available: {e}\")\n",
    "            print(\"Continuing with RAG only...\")\n",
    "        \n",
    "        self.render_ui_with_rag()\n",
    "\n",
    "    def run_with_rag(self):\n",
    "        \"\"\"Run chatbot with RAG and optional MCP tools.\"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # Jupyter notebook case\n",
    "            asyncio.ensure_future(self.run_with_rag_async())\n",
    "        else:\n",
    "            loop.run_until_complete(self.run_with_rag_async())\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources\"\"\"\n",
    "        if self.vectorizer:\n",
    "            self.vectorizer.close()\n",
    "        super().cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a331169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connected to PostgreSQL database\n",
      "‚úì RAG vectorizer initialized and connected to database\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MCP server at: http://localhost:8080/sse\n",
      "‚ö†Ô∏è  MCP not available: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "Continuing with RAG only...\n",
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and run RAG chatbot\n",
    "chat_bot_with_rag = ChatBotWithRAG()\n",
    "chat_bot_with_rag.run_with_rag()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
